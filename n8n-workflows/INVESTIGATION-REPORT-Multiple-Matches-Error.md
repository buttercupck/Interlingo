# n8n Workflow Investigation Report
## Error: "Cannot assign to read only property 'name' of object 'Error: Multiple matches'"

**Date:** 2026-02-02
**Workflow:** gcal-to-interlingo-upsert-path2
**Investigator:** Dev Patel (Senior Debugging Engineer)
**Server:** n8n.interlingo.augeo.one (Docker: n8n-interlingo)

---

## EXECUTIVE SUMMARY

**ROOT CAUSE IDENTIFIED:** The error "Multiple matches" is likely NOT happening due to Supabase returning multiple rows, but rather due to a deeper issue with how n8n's Supabase node handles the `getAll` operation with `limit: 1` when combined with error scenarios.

After comprehensive investigation:
1. âœ… **Database validated** - No duplicate language entries found (UNIQUE constraint working)
2. âš ï¸ **Data quality issue found** - Duplicate "Amharic"/"AMHARIC" entries exist
3. ðŸ” **Configuration issue** - Using `getAll` with `limit: 1` instead of single-row retrieval
4. âš ï¸ **Missing language** - Test data shows "Mam" language not in database

---

## INVESTIGATION FINDINGS

### 1. Database Analysis

**Supabase Instance:** `https://anqfdvyhexpxdpgbkgmd.supabase.co`

**Languages Table Check:**
```bash
Total languages: 63
Duplicate case variations found:
  - "Amharic" (id: 90862e52-b1f1-4edd-8720-a52ab879e450)
  - "AMHARIC" (id: cdb75528-16be-4635-ad53-c04c1909a956)
```

**Schema Validation:**
- âœ… UNIQUE constraint exists on `languages.name` column
- âœ… Constraint is working (prevents exact duplicates)
- âš ï¸ Case-sensitive duplicates still possible ("Amharic" â‰  "AMHARIC")

### 2. Workflow Configuration Analysis

**Current "Lookup Language ID" Node Configuration:**
```json
{
  "operation": "getAll",
  "tableId": "languages",
  "limit": 1,
  "filterType": "string",
  "filterString": "name=eq.={{ $json.language.trim() }}"
}
```

**Issues Identified:**
1. **Operation mismatch**: Using `getAll` (array operation) when expecting single result
2. **No case normalization**: `.trim()` but no `.toLowerCase()` or `.toUpperCase()`
3. **Error handling**: Relying on downstream code node to handle empty/multiple results
4. **Limit semantics**: `limit: 1` still returns an array, not a single object

### 3. Test Data Analysis

**Pinned test event:** "Spanish/Mam - Ma M ZOOM" (Event ID: _8go3ig9g...)
- Language string: "Spanish/Mam"
- Split into: ["Spanish", "Mam"]
- **Spanish**: âœ… Found in database (id: 03faa119-9f1b-4bc9-9085-2c06ea546442)
- **Mam**: âŒ NOT in database (returns empty array)

### 4. Error Context

The error message "Cannot assign to read only property 'name' of object 'Error: Multiple matches'" suggests:
- n8n is trying to modify an Error object's properties
- The "Multiple matches" is the error message from Supabase or n8n's internal validation
- The error occurs in "Process Language Lookup" node when accessing `$input.all()`

**Hypothesis:** The Supabase node itself may be throwing this error before the code node even executes, possibly due to:
- Internal validation when `getAll` with `limit: 1` returns unexpected results
- Version mismatch between n8n Supabase node and Supabase API
- Edge case handling in n8n's Supabase node implementation

---

## ROOT CAUSE ANALYSIS

### Primary Cause
**n8n Supabase Node Configuration Issue:**
The workflow uses `operation: "getAll"` which is designed for retrieving multiple rows. When combined with `limit: 1`, this creates ambiguous behavior:
- The operation expects to return an array of results
- The limit restricts to 1 result
- If Supabase's internal logic detects potential multiple matches BEFORE applying the limit, it may throw "Multiple matches" error
- This is especially likely if the query filter is ambiguous or if there's a race condition with data changes

### Secondary Causes
1. **Missing language entries** - "Mam" not in database causes workflow failures
2. **Case sensitivity issues** - "Amharic" vs "AMHARIC" creates data quality problems
3. **No defensive programming** - No case normalization before querying

### Error Propagation Path
```
GCal Event â†’ Parse Event â†’ Lookup Language ID [FAILS HERE] â†’ Process Language Lookup
                                                â†“
                                    "Multiple matches" error thrown
                                                â†“
                            n8n tries to process error as data
                                                â†“
                        "Cannot assign to read only property 'name'" error
```

---

## RECOMMENDED SOLUTIONS

### Solution 1: Fix Supabase Node Operation (RECOMMENDED)
**Impact:** High | **Complexity:** Low | **Risk:** Low

Change the Supabase node from `getAll` to use proper single-row semantics:

**Option A: Use PostgREST's single row endpoint**
```json
{
  "operation": "getAll",
  "tableId": "languages",
  "limit": 1,
  "filterType": "string",
  "filterString": "name=eq.={{ $json.language.trim() }}",
  "returnAll": false
}
```

**Option B: Update to custom query (if n8n supports)**
Use Supabase's `.single()` modifier or `.maybeSingle()`:
```json
{
  "operation": "executeQuery",
  "query": "SELECT * FROM languages WHERE name = {{ $json.language.trim() }} LIMIT 1"
}
```

**Option C: Change error handling in code node**
Update "Process Language Lookup" to handle the error gracefully:
```javascript
try {
  const languageLookup = $input.all();
  // existing logic
} catch (error) {
  // Handle "Multiple matches" error
  if (error.message && error.message.includes('Multiple matches')) {
    return [{
      json: {
        error: true,
        error_type: 'multiple_language_matches',
        error_message: `Multiple language entries found for "${parsed.language}" - database cleanup needed`,
        gcal_event_id: parsed.gcal_event_id,
        start_time: parsed.start_time,
        language: parsed.language
      }
    }];
  }
  throw error; // Re-throw other errors
}
```

### Solution 2: Database Cleanup and Normalization (REQUIRED)
**Impact:** High | **Complexity:** Medium | **Risk:** Medium

**Step 1: Identify all case duplicates**
```sql
SELECT
  LOWER(name) as normalized_name,
  array_agg(name) as variations,
  array_agg(id::text) as ids,
  COUNT(*) as count
FROM languages
GROUP BY LOWER(name)
HAVING COUNT(*) > 1;
```

**Step 2: Consolidate duplicates**
```sql
-- Example: Merge "AMHARIC" into "Amharic"
-- 1. Find all references to the duplicate
SELECT table_name, column_name
FROM information_schema.columns
WHERE column_name LIKE '%language_id%';

-- 2. Update foreign key references
UPDATE client_requests
SET language_id = '90862e52-b1f1-4edd-8720-a52ab879e450'  -- "Amharic"
WHERE language_id = 'cdb75528-16be-4635-ad53-c04c1909a956'; -- "AMHARIC"

UPDATE interpreter_languages
SET language_id = '90862e52-b1f1-4edd-8720-a52ab879e450'
WHERE language_id = 'cdb75528-16be-4635-ad53-c04c1909a956';

-- 3. Delete duplicate
DELETE FROM languages WHERE id = 'cdb75528-16be-4635-ad53-c04c1909a956';
```

**Step 3: Add case-insensitive unique constraint**
```sql
-- Create case-insensitive unique index
CREATE UNIQUE INDEX languages_name_lower_unique
ON languages (LOWER(name));

-- Or use a generated column (PostgreSQL 12+)
ALTER TABLE languages
ADD COLUMN name_normalized TEXT GENERATED ALWAYS AS (LOWER(name)) STORED;

CREATE UNIQUE INDEX languages_name_normalized_unique
ON languages (name_normalized);
```

### Solution 3: Add Missing Languages (REQUIRED)
**Impact:** High | **Complexity:** Low | **Risk:** Low

Add "Mam" and any other missing languages:
```sql
INSERT INTO languages (name)
VALUES ('Mam')
ON CONFLICT (name) DO NOTHING;

-- Verify
SELECT * FROM languages WHERE name = 'Mam';
```

**Find all missing languages from GCal events:**
```bash
# SSH to n8n server
ssh deploy@n8n.interlingo.augeo.one

# Check n8n execution logs for missing language errors
docker logs n8n-interlingo 2>&1 | grep "Language.*not found" | sort | uniq
```

### Solution 4: Add Case Normalization to Workflow (RECOMMENDED)
**Impact:** Medium | **Complexity:** Low | **Risk:** Very Low

Update workflow to normalize language names before lookup:

**In "Parse Event" node (line 9-10), add normalization:**
```javascript
// Existing code...
const languages = languageString.split('/').map(l => l.trim()).filter(l => l.length > 0);

// ADD: Normalize to Title Case
function toTitleCase(str) {
  return str.toLowerCase().split(' ').map(word =>
    word.charAt(0).toUpperCase() + word.slice(1)
  ).join(' ');
}

return languages.map(language => {
  const normalizedLanguage = toTitleCase(language); // Normalize here
  const fingerprintRaw = `${startTime}|${orgAbbrev}|${normalizedLanguage}`;
  const fingerprintHash = simpleHash(fingerprintRaw);

  return {
    json: {
      gcal_event_id: event.id,
      start_time: startTime,
      end_time: endTime,
      language: normalizedLanguage, // Use normalized
      // ... rest of fields
    }
  };
});
```

---

## IMPLEMENTATION PRIORITY

### IMMEDIATE (Do First)
1. âœ… **Database cleanup** - Merge "AMHARIC" â†’ "Amharic"
2. âœ… **Add missing languages** - Insert "Mam" and others
3. âœ… **Update Supabase node error handling** - Add try/catch in Process Language Lookup

### SHORT-TERM (Do This Week)
4. âœ… **Add case normalization** - Update Parse Event node
5. âœ… **Add case-insensitive constraint** - Prevent future duplicates
6. âœ… **Test workflow** - Run with pinned test data

### MEDIUM-TERM (Do This Month)
7. âšª **Audit all lookup nodes** - Check Org and Interpreter lookups for same issues
8. âšª **Add monitoring** - Alert on lookup failures
9. âšª **Document language additions** - Create process for adding new languages

---

## TESTING CHECKLIST

After implementing fixes, test these scenarios:

- [ ] Spanish (exact match, common case)
- [ ] spanish (lowercase variation)
- [ ] SPANISH (uppercase variation)
- [ ] Spanish/Mam (dual language with new entry)
- [ ] Amharic (after duplicate cleanup)
- [ ] NonexistentLanguage (verify error handling)

---

## ADDITIONAL NOTES

### n8n Version
Running n8n v2.6.3 (from container logs)

### Supabase Node Behavior
The exact behavior of `getAll` with `limit: 1` may vary by n8n version. Consider:
- Reviewing n8n Supabase node source code
- Checking n8n community forums for similar issues
- Upgrading n8n if bug is fixed in newer version

### Long-term Recommendations
1. **Standardize all reference data** - Create naming conventions (Title Case for languages, UPPERCASE for org abbreviations)
2. **Add data validation** - UI-level validation before inserting languages/orgs
3. **Consider lookup caching** - Store language_id mappings in n8n static data for performance
4. **Implement fuzzy matching** - Use Levenshtein distance for typo tolerance

---

## CONCLUSION

The "Multiple matches" error is likely caused by a combination of:
1. Ambiguous Supabase node configuration (`getAll` + `limit: 1`)
2. Case-sensitive duplicates in database
3. Poor error handling when edge cases occur

**Recommended immediate action:**
1. Clean up duplicate "AMHARIC" entry
2. Add missing "Mam" language
3. Add error handling in "Process Language Lookup" node
4. Test with pinned data

**Success metrics:**
- Workflow processes test event without errors
- All languages in GCal events have corresponding database entries
- No case-variation duplicates in languages table

---

**Report prepared by:** Dev Patel, Senior Debugging Engineer
**Status:** Investigation Complete - Ready for Implementation
